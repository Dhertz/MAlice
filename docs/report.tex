\documentclass[11pt, notitlepage]{report}

\usepackage[tmargin=2cm, lmargin=2cm, rmargin=2cm, bmargin=3cm]{geometry}
\usepackage{parskip}
\usepackage[latin1]{inputenc}
\usepackage{graphicx}
\usepackage{wrapfig}

\begin{document}


\title{MAlice Report}
\author{Owen Davies, Daniel Hertz, Charlie Hothersall}
\date{\today}

\maketitle

\section*{The Product}
Our compiler is largely successful and generally meets the criteria set out in the specification for the project. In particular, the syntax and semantic analysis components of the compiler are extremely sound. There are some small bugs in the code generation portion of the compiler, which we would have liked to fix if we had had more time on the project.

We think that it would be fairly easy to develop our compiler further. Using a class based system for our AST (with a seperate class for each node) means that it is very easy to add new language features - one can simply add new nodes that extend the \texttt{ASTNode} class, and add methods in the \texttt{TreeWalker} class to create the new node objects. Similarly, in the code generation stage, one can just add new methods to \texttt{ASTVisitor} class to process the new AST node types. This is one of the benefits of using the visitor pattern to generate code from an AST.

We chose to write our compiler in C++. We chose C++ because we wanted to create an object based solution (to take advantage of design patterns like the visitor pattern), and we also wanted to use this project as an opportunity to develop out skills in the language, something we definitely feel we have achieved. This is also partly the reason we chose to compile down to ARM Assembly - ARM is an architecture we are all interested in (and an architecture that is growing in popularity very quickly), but not one which we knew much about. We saw this as a good opportunity to learn more about it. Additionally, we wanted to somehow involve the Raspberry Pi we had available to use in the project, and compiling down to ARM was therefore required.

Throughout the project we used the \texttt{boost} C++ libraries, in particular \texttt{boost::shared\_ptr} to make memory management simpler (we were initially using raw pointers, but managing these in the semantic analysis stage became very complex). Whilst using \texttt{shared\_ptr}s definitely made the code simpler, they are quite a lot slower than raw pointers (due to the reference counting that is built in to a smart pointer). In this area, we decided to sacrifice performance for simplicity.

\section*{Design Choices}
For the lexical and syntax analysis sections of the compiler, we decided to use the lexer and parser generator tool \emph{ANTLRv3}. Our success with this tool was mixed: the tool has a very steep learning curve, and at the start of the project progress was very slow. \emph{ANTLR} was originally written with Java as a language target. We wanted to use C++, and it soon became apparent that the documentation was distinctly lacking for the C++ target. In fact, the \emph{ANTLR} C++ target was so limited that we decided to use the C target instead, which had more features and fair bit more documentation. We could then write C++ code on top of the generated C code.

This approach proved far more successful, and soon we were making rapid progress through the lexing and parsing of the MAlice language. Once one gets to grips with \emph{ANTLR}, it is an incredibly powerful tool. In particular, two features would prove to be very useful:
\begin{itemize}
\item \textbf{Syntactic Predicates} - This feature allows you to tell \emph{ANLTR} to look ahead to the next n tokens and check if the input confirms to this subrule. If it doesn't, move to the next rule. This is written as such:
\begin{center}
	\texttt{rule: (A B C) => x | y}
\end{center}
The above rule says ``If \texttt{(A B C)} holds for the current token stream then do \texttt{x}, otherwise do \texttt{y}''. This feature allowed us to get around any ambiguity in the MAlice language very easily.

\item \textbf{Rewrite Rules} - This allows the user to map and input stream to an output tree. This comes in very handy when creating a parse tree, as you can create imaginary nodes for you parse tree that come in very useful when walking the tree later on:
\begin{center}
	\texttt{rule: a b `.' -> \textasciicircum(TOK a b)}
\end{center}
This creates a new sub-tree in the parse tree with \texttt{TOK} as the root node. This rewrite system is also very useful for defining operator precedence, since you can make the binary operator the root node of it's own subtree, and the expression automatically has the correct precedence.
\end{itemize}
If we had to go back and redo the project, we would have looked at more options for lexer and parser generation in C++, to avoid the inital few days of wrestling with \emph{ANTLR}, trying to get it to behave as we wished. However, it is an excellent tool once you start to understand it properly.

We decided to create the semantic analysis part of our compiler by hand. We could have perhaps used \texttt{ANTLR} for this section as well: there exists the tools to do that sort of thing using \emph{ANTLR}, but we decided that we didn't have enough time to learn these somewhat complex aspects of tool well enough to actually create a good solution using them. Additionally, the lectures we had been given on semantic analysis were comprehensive enough so that we felt equipped to hand craft this portion of the compiler.

\section*{Beyond the Specification}
\subsection*{Optimisations}
We have made a number of optimisations in our compiler:
\begin{itemize}
\item \textbf{Removing unecessary \texttt{mov} instructions and sequences} - any instructions of the form \texttt{mov ri ri} and instruction sequences of the form \texttt{mov ri rx ... mov ry ri} are removed.
\item \textbf{Removing uncalled functions} - any functions with are not called in the program (and hence are dead code) are removed from the program code.
\item \textbf{Removing empty statement blocks} - any \texttt{perhaps}, \texttt{either}, or \texttt{eventually} code blocks without a body are removed. This removal is done before code generation to avoid generating code which will later be removed.
\item \textbf{Register allocation} - Before the instructions for an expression are generated, we apply a weighting algorithm to the expression tree which attempts to work out the ``cost'' of each sub expression, and therefore the most sensible order to allocate registers for the expression (we want to generate code for the ``cheapest'' expression first). Expressions like \texttt{(2 * x + y) + x} would normally get evaluated left to right, but it's more efficient to generate code for the right hand side first in this case. We also make sure that two registers never hold the same expression, to try and prevent the program from running out of registers for as long as possible.
\end{itemize}

\subsection*{Extension - Hardware GPIO in MAlice}
Since we had decided to compile down to ARM assembly, and we had a Raspberry Pi to hand, we thought that it would be fun to extend malice to control the General Purpose I/O pins on the Pi. The following MAlice code shows the new commands we have extended the MAlice language to include:

\begin{verbatim}
###include io
The looking-glass hatta()
opened
    x was a number.

    The Caterpillar blew out 4 smoke rings. ### make IO4 an output pin
    The Caterpillar inhaled 17 times.       ### make IO17 an input pin

    Alice discovered x through door 17.     ### x = 0 or 1, depending on IO17's value

    17 said Tweedledee.                     ### set pin IO17 high
    Alice slept for 100 milliseconds.       ### sleep for 100ms
    17 said Tweedledum.                     ### set pin IO17 low
closed
\end{verbatim}

The implementation of these features required changes to almost every element in the project, and this is where our design came into play, as it took minimal effort to add commands to the language and get some output. This meant we could spend the majority of the time getting to grips with the Raspberry Pi's (somewhat undocumented) GPIO controller. We implemented setting up the virtual memory to the controllers registers in C, and linked the generated assembly with this library at compile time. This is why \texttt{\#\#\#include io} is needed as the first line of the file, to tell the preprocessor to change the assemble command.

\begin{wrapfigure}{r}{0.35\textwidth}
  \vspace{-20pt}
  \begin{center}
    \includegraphics[width=0.35\textwidth]{IMG_9457.JPG}
    \caption{Charlie's awesome circuit}
  \end{center}
  \vspace{-10pt}
\end{wrapfigure}
After the basics had been implemented, we then had lots of fun trying to communicate with the GPIO controller on the lowest level. Our first proof of concept was done in python, with just a LED flashing:

We then wrote out C library to include methods to set pins as input and output, as well as set them high and low. As well as that, we implemented an assembly method to pause the program, so we could then blink an LED.

\begin{wrapfigure}{r}{0.35\textwidth}
  \vspace{-20pt}
  \begin{center}
    \includegraphics[width=0.35\textwidth, angle=270]{IMG_9458.JPG}
    \caption{}
  \end{center}
  \vspace{-10pt}
\end{wrapfigure}
We then put all these elements together, and to test the new commands, we built some electronics to interface with an Xbox 360, and wrote some malice files to play songs from 'Guitar Hero 3'. During our first effort, the timing of the notes was slightly off, so we asked to use Robotics Society's digital oscilloscope to test our timing code. We then proved that it was millisecond perfect:

This lead to us changing the malice files by eye to account for the mysterious delays in the code, and we had Alice up and rocking.

\end{document}
